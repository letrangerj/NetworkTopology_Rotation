#!/usr/bin/env Rscript

# Phase 04 - Step 03: Modularity Analysis (replicate Louvain + minimal output)
# -------------------------------------------------------------------------
# Purpose:
#   - Perform replicate Louvain community detection on the combined bipartite graph
#     (functional-group nodes + pyoverdine nodes) using an undirected representation.
#   - Produce replicate partitions, modularity scores, a co-assignment matrix,
#     a consensus partition derived from the co-assignment matrix.
#   - Minimal output mode: text summary and figures only (no RDS/CSV files)
#
# Outputs:
#   - figures/network_topology/step03_modularity_histogram.pdf
#   - figures/network_topology/step03_coassignment_heatmap.pdf
#   - docs/phase_04/logs/step03_modularity_summary.txt
#
# Notes:
#   - This script uses igraph's Louvain implementation (`cluster_louvain`) on an undirected,
#     unweighted graph that contains both FG and pyov nodes.
#
# Usage:
#   Rscript scripts/phase_04_topology/03_modularity_analysis.R
#
# Author: automated assistant
# Date: autogenerated

suppressPackageStartupMessages({
  library(igraph)
  library(Matrix)
  library(pheatmap)
  library(jsonlite)
  library(dplyr)
  library(ggplot2)
})

# -----------------------------
# Helper utilities & parameters
# -----------------------------
safe_dir_create <- function(path) if (!dir.exists(path)) dir.create(path, recursive = TRUE, showWarnings = FALSE)
timestamp <- function() format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")

# Directories
safe_dir_create("figures/network_topology")
safe_dir_create("docs/phase_04/logs")

# Load parameter manifest if present, else fallback to defaults
manifest_json <- "docs/phase_04/parameters_manifest.json"
manifest <- list()
if (file.exists(manifest_json)) {
  manifest <- tryCatch(fromJSON(manifest_json), error = function(e) {
    message("Could not parse manifest JSON; using defaults")
    list()
  })
}

# Default parameters (override from manifest if present)
master_seed <- if (!is.null(manifest$master_seed)) manifest$master_seed else 2025
mod_reps_pilot <- if (!is.null(manifest$modularity_replicates$pilot)) manifest$modularity_replicates$pilot else 20
n_replicates <- 200

# Filepaths for Phase 3 artifacts
adj_prod_fg_path <- "data/interim/adj_production_FG_agentsxpyov_conservative.rds"
adj_util_fg_path <- "data/interim/adj_utilization_FG_pyovxagents_conservative.rds"
edges_fg_csv_path <- "data/interim/edges_functional_groups_conservative.csv"
fg_nodes_path <- "data/interim/nodes_functional_groups_conservative.rds"
pyov_nodes_path <- "data/interim/nodes_pyoverdines_conservative.rds"

# Check input existence
required_paths <- c(adj_prod_fg_path, adj_util_fg_path, fg_nodes_path, pyov_nodes_path, edges_fg_csv_path)
for (p in required_paths) {
  if (!file.exists(p)) stop("Required input missing: ", p, "\nRun Phase 03 steps or ensure files are present.")
}

# -----------------------------
# Load inputs
# -----------------------------
message("Loading adjacency matrices and node tables...")
adj_prod_fg <- readRDS(adj_prod_fg_path)     # FG (rows) x PYO (cols)
adj_util_fg <- readRDS(adj_util_fg_path)     # PYO (rows) x FG (cols)
fg_nodes    <- readRDS(fg_nodes_path)
pyov_nodes  <- readRDS(pyov_nodes_path)

# Build undirected edge list for combined bipartite graph.
message("Building combined undirected bipartite graph (FG + PYO nodes)...")

# Construct edges from production adjacency matrix
prod_summary <- summary(adj_prod_fg)
fg_ids <- rownames(adj_prod_fg)
pyo_ids <- colnames(adj_prod_fg)
undirected_edges <- tibble(
  from = fg_ids[prod_summary$i],
  to   = pyo_ids[prod_summary$j]
) %>%
  rowwise() %>%
  mutate(n1 = min(c(from, to)), n2 = max(c(from, to))) %>%
  ungroup() %>%
  transmute(from = n1, to = n2) %>%
  distinct()

# Build vertex list: union of FG agent ids and pyov node ids
vertex_ids <- unique(c(undirected_edges$from, undirected_edges$to))

# Create igraph undirected graph
g_combined <- graph_from_data_frame(undirected_edges, directed = FALSE, vertices = data.frame(name = vertex_ids, stringsAsFactors = FALSE))

# Annotate vertex types
V(g_combined)$type <- ifelse(V(g_combined)$name %in% fg_nodes$agent_id, "fg",
                             ifelse(V(g_combined)$name %in% pyov_nodes$node_id, "pyov", "unknown"))

# Remove any self-loops or duplicated edges
g_combined <- simplify(g_combined, remove.multiple = TRUE, remove.loops = TRUE)

message(sprintf("Graph built: %d vertices (%d FG, %d PYO) and %d edges",
                vcount(g_combined),
                sum(V(g_combined)$type == "fg"), sum(V(g_combined)$type == "pyov"),
                ecount(g_combined)))

# -----------------------------
# Replicate Louvain clustering
# -----------------------------
message("Running replicate Louvain community detection...")

# Deterministic replicate seeds derived from master_seed
set.seed(master_seed)
replicate_seeds <- master_seed + seq_len(n_replicates)

# Prepare storage
node_names <- V(g_combined)$name
n_nodes <- length(node_names)
memberships_mat <- matrix(NA_integer_, nrow = n_nodes, ncol = n_replicates,
                          dimnames = list(node_names, paste0("rep_", seq_len(n_replicates))))
modularity_scores <- numeric(n_replicates)
partition_list <- vector("list", length = n_replicates)

# Run Louvain replicates with vertex permutation for stochasticity
for (r in seq_len(n_replicates)) {
  seed_r <- replicate_seeds[r]
  set.seed(seed_r)

  # Random permutation of vertices
  perm <- sample(seq_len(n_nodes))
  g_perm <- permute(g_combined, perm)

  # Run Louvain (on unweighted graph)
  cl <- cluster_louvain(g_perm)

  # Map membership back to original vertex order
  membership_perm <- cl$membership
  membership_orig_order <- integer(length(membership_perm))
  membership_orig_order[perm] <- membership_perm

  # Save membership and modularity score
  memberships_mat[, r] <- membership_orig_order
  modularity_scores[r] <- modularity(g_perm, cl$membership)
  partition_list[[r]] <- membership_orig_order

  if (r %% 10 == 0) {
    message(sprintf("Replicate %d/%d: seed=%d modules=%d modularity=%.4f",
                    r, n_replicates, seed_r, length(unique(membership_orig_order)), modularity_scores[r]))
  }
}

# -----------------------------
# Co-assignment matrix
# -----------------------------
message("Computing co-assignment matrix across replicates...")

# coassignment[i,j] = proportion of replicates where node i and node j are in same module
coassign <- matrix(0, nrow = n_nodes, ncol = n_nodes, dimnames = list(node_names, node_names))

for (i in seq_len(n_nodes)) {
  for (j in i:n_nodes) {
    eq_count <- sum(memberships_mat[i, ] == memberships_mat[j, ])
    prop <- eq_count / n_replicates
    coassign[i, j] <- prop
    coassign[j, i] <- prop
  }
}

# -----------------------------
# Consensus partitioning
# -----------------------------
message("Deriving consensus partition from co-assignment matrix...")

# Build weighted graph from coassignment matrix
g_coassign <- graph_from_adjacency_matrix(coassign, mode = "undirected", weighted = TRUE, diag = FALSE)

# Run Louvain on coassignment weighted graph
set.seed(master_seed + 9999)
cons_cl <- cluster_louvain(g_coassign, weights = E(g_coassign)$weight)
cons_membership <- cons_cl$membership
names(cons_membership) <- V(g_coassign)$name

consensus_result <- list(
  membership = cons_membership,
  modularity = cons_cl$modularity,
  n_modules = length(unique(cons_membership))
)

message(sprintf("Consensus partition: modules=%d modularity=%.4f",
                consensus_result$n_modules, consensus_result$modularity))

# -----------------------------
# Module size summary
# -----------------------------
module_sizes_df <- as.data.frame(table(cons_membership), stringsAsFactors = FALSE)
colnames(module_sizes_df) <- c("module", "size")
module_sizes_df$module <- as.integer(as.character(module_sizes_df$module))
module_sizes <- module_sizes_df %>% arrange(desc(size))

# -----------------------------
# Visualizations
# -----------------------------
message("Creating modularity histogram and co-assignment heatmap...")

# Histogram of modularity scores
p_hist <- ggplot(data.frame(modularity = modularity_scores), aes(x = modularity)) +
  geom_histogram(bins = max(10, round(n_replicates/2)), fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = consensus_result$modularity, color = "red", linetype = "dashed", linewidth = 1) +
  labs(title = "Louvain modularity scores across replicates",
       x = "Modularity (Q)", y = "Frequency") +
  annotate("text", x = consensus_result$modularity, y = 0,
           label = sprintf("Consensus: %.3f", consensus_result$modularity),
           vjust = -1, color = "red", size = 3) +
  theme_minimal()

ggsave("figures/network_topology/step03_modularity_histogram.pdf", p_hist, width = 7, height = 4)

# Co-assignment heatmap (cluster rows/cols to highlight modules)
order_by_consensus <- names(sort(cons_membership))
coassign_ordered <- coassign[order_by_consensus, order_by_consensus]

annotation_df <- data.frame(Module = factor(cons_membership[order_by_consensus]))
rownames(annotation_df) <- order_by_consensus

p_heatmap <- pheatmap::pheatmap(coassign_ordered,
                 cluster_rows = FALSE,
                 cluster_cols = FALSE,
                 show_rownames = FALSE,
                 show_colnames = FALSE,
                 annotation_row = annotation_df,
                 annotation_col = annotation_df,
                 color = colorRampPalette(c("white", "orange", "red"))(50),
                 main = "Co-assignment matrix (ordered by consensus modules)",
                 silent = TRUE)

ggsave("figures/network_topology/step03_coassignment_heatmap.pdf", p_heatmap$gtable,
       width = 8, height = 7)

# -----------------------------
# Generate comprehensive summary
# -----------------------------
summary_lines <- c(
  "Phase 04 - Step 03: Modularity Analysis Summary (Minimal Output)",
  paste("Timestamp:", timestamp()),
  "",
  sprintf("Network Summary:"),
  sprintf("  Vertices: %d (%d FG, %d PYO)", vcount(g_combined),
          sum(V(g_combined)$type == "fg"), sum(V(g_combined)$type == "pyov")),
  sprintf("  Edges: %d", ecount(g_combined)),
  sprintf("  Density: %.4f", graph.density(g_combined)),
  "",
  sprintf("Replicate Analysis (n = %d replicates):", n_replicates),
  sprintf("  Modularity mean ± SD: %.4f ± %.4f", mean(modularity_scores), sd(modularity_scores)),
  sprintf("  Modularity range: %.4f - %.4f", min(modularity_scores), max(modularity_scores)),
  sprintf("  Mean modules per replicate: %.1f", mean(sapply(partition_list, function(x) length(unique(x))))),
  "",
  sprintf("Consensus Partition:"),
  sprintf("  Number of modules: %d", consensus_result$n_modules),
  sprintf("  Consensus modularity: %.4f", consensus_result$modularity),
  "",
  "Module Sizes (descending order):",
  paste(sprintf("  Module %d: %d nodes", module_sizes$module, module_sizes$size), collapse = "\n"),
  "",
  sprintf("Largest Module Properties:"),
  sprintf("  Size: %d nodes (%.1f%% of network)", max(module_sizes$size),
          100 * max(module_sizes$size) / vcount(g_combined)),
  "",
  "Interpretation:",
  "  • High modularity (>0.3): Strong community structure present",
  "  • Low modularity (<0.1): Weak or no community structure",
  "  • High replicate variance: Algorithm instability or network ambiguity",
  "  • Large dominant module: Possible hub-and-spoke structure",
  "",
  "Outputs:",
  "  • figures/network_topology/step03_modularity_histogram.pdf",
  "  • figures/network_topology/step03_coassignment_heatmap.pdf",
  "  • docs/phase_04/logs/step03_modularity_summary.txt",
  "",
  "Note: RDS/CSV files intentionally omitted (compact output mode)",
  "",
  "Ready for subsequent topology analyses."
)

writeLines(summary_lines, "docs/phase_04/logs/step03_modularity_summary.txt")

# Display final summary
cat(paste(summary_lines, collapse = "\n"), "\n")

# Save session info
writeLines(capture.output(sessionInfo()), "docs/phase_04/logs/step03_session_info.txt")

cat("\nStep 03 complete. Modularity analysis finished.\n")
message(sprintf("Summary saved to: docs/phase_04/logs/step03_modularity_summary.txt"))
cat(sprintf("Visualizations: %s\n", paste(list.files("figures/network_topology", pattern = "^step03_.*\\.pdf$", full.names = TRUE), collapse = ", ")))
