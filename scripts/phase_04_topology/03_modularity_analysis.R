#!/usr/bin/env Rscript

# Phase 04 - Step 03: Bipartite Modularity Analysis
# -------------------------------------------------
# Purpose:
#   Perform bipartite-aware community detection on FG x pyoverdine incidence matrix
#   using bipartite::computeModules() with replicate analysis and stability assessment.
#   Replaces the problematic Louvain-on-combined-graph + coassignment consensus approach.
#
# Responsibilities:
#   1. Build proper bipartite incidence matrix (FG x pyov)
#   2. Run bipartite::computeModules() with multiple replicates and random seeds
#   3. Assess partition stability using Adjusted Rand Index (ARI)
#   4. Derive consensus partition using medoid approach (not Louvain-on-coassignment)
#   5. Generate bipartite modularity metrics and module characterization
#   6. Save replicate partitions and consensus results
#   7. Create visualizations showing module structure and stability
#
# Outputs:
#   - results/phase_04/modularity/modules_fg_replicates.rds
#   - results/phase_04/modularity/modules_fg_consensus.rds
#   - results/phase_04/modularity/modularity_stability.rds
#   - figures/network_topology/modularity_bipartite_consensus.pdf
#   - figures/network_topology/modularity_stability_metrics.pdf
#   - docs/phase_04/logs/step03_modularity_bipartite_summary.txt
#
# Usage:
#   Rscript scripts/phase_04_topology/03_modularity_analysis.R
#
# Dependencies: bipartite, Matrix, ggplot2, dplyr, jsonlite, mclust
#
# Author: automated assistant
# Date: autogenerated

suppressPackageStartupMessages({
  library(bipartite)
  library(Matrix)
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  library(jsonlite)
  library(mclust) # For adjustedRandIndex
})

# -----------------------------
# Helper functions
# -----------------------------
safe_dir_create <- function(path) {
  if (!dir.exists(path)) dir.create(path, recursive = TRUE, showWarnings = FALSE)
}

timestamp <- function() format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")

# Compute Adjusted Rand Index between two partitions
compute_ari <- function(partition1, partition2) {
  if (length(partition1) != length(partition2)) {
    return(NA)
  }
  return(mclust::adjustedRandIndex(partition1, partition2))
}

# Find medoid partition (partition with minimum average ARI distance to all others)
find_medoid_partition <- function(partitions_matrix) {
  n_reps <- ncol(partitions_matrix)
  if (n_reps <= 1) {
    return(list(medoid_idx = 1, avg_ari_to_medoid = 1))
  }

  avg_ari_distances <- numeric(n_reps)

  for (i in seq_len(n_reps)) {
    ari_values <- numeric(n_reps)
    for (j in seq_len(n_reps)) {
      if (i != j) {
        ari_values[j] <- compute_ari(partitions_matrix[, i], partitions_matrix[, j])
      } else {
        ari_values[j] <- 1 # self-ARI is 1
      }
    }
    avg_ari_distances[i] <- mean(ari_values, na.rm = TRUE)
  }

  medoid_idx <- which.max(avg_ari_distances) # Highest average ARI = closest to others
  return(list(medoid_idx = medoid_idx, avg_ari_to_medoid = avg_ari_distances[medoid_idx]))
}

# Extract module assignments from bipartite::computeModules result
extract_bipartite_modules <- function(modules_result, fg_names, pyov_names) {
  # Default fallback (used only if extraction fails)
  fallback <- list(
    fg_modules = rep(1L, length(fg_names)),
    pyov_modules = rep(1L, length(pyov_names)),
    modularity = 0,
    n_modules = 1L,
    success = FALSE
  )

  if (is.null(modules_result)) {
    return(fallback)
  }

  fg_modules <- NULL
  pyov_modules <- NULL
  modularity_val <- NA_real_
  used_fallback <- FALSE


  if (isS4(modules_result)) {
    # Primary robust path: use bipartite::listModuleInformation() and align by names
    info <- tryCatch(bipartite::listModuleInformation(modules_result), error = function(e) NULL)
    if (is.list(info)) {
      # Try to extract named membership vectors for both levels
      # Search recursively for numeric vectors and align by names if possible
      flatten_list <- function(x) {
        if (is.list(x)) {
          unlist(lapply(x, flatten_list), recursive = FALSE)
        } else {
          list(x)
        }
      }
      candidates <- flatten_list(info)

      # Helper: pick a vector that matches by names first, else by length
      pick_membership <- function(cands, wanted_names, wanted_len) {
        # 1) Prefer named numeric vectors that cover wanted_names
        for (v in cands) {
          if (is.numeric(v) && !is.null(names(v))) {
            common <- intersect(names(v), wanted_names)
            if (length(common) >= wanted_len) {
              # Align by names
              vv <- as.integer(v[wanted_names])
              if (length(vv) == wanted_len && all(!is.na(vv))) {
                return(vv)
              }
            }
          }
        }
        # 2) Fall back to exact-length numeric vectors, assume order matches
        for (v in cands) {
          if (is.numeric(v) && is.null(names(v)) && length(v) == wanted_len) {
            return(as.integer(v))
          }
        }
        NULL
      }

      # Attempt to identify FG and PYO memberships
      v_numeric <- Filter(is.numeric, candidates)
      fg_try <- pick_membership(v_numeric, fg_names, length(fg_names))
      py_try <- pick_membership(v_numeric, pyov_names, length(pyov_names))

      if (!is.null(fg_try) && !is.null(py_try)) {
        fg_modules <- fg_try
        pyov_modules <- py_try
      }
      # Extract likelihood/modularity-like score if present
      if (is.null(modularity_val)) {
        if (!is.null(info$likelihood) && is.numeric(info$likelihood)) modularity_val <- as.numeric(info$likelihood)
        if (!is.null(info$ll) && is.numeric(info$ll)) modularity_val <- as.numeric(info$ll)
      }
    }

    # Secondary path: slot-based extraction
    if (is.null(fg_modules) || is.null(pyov_modules)) {
      mods_slot <- tryCatch(slot(modules_result, "modules"), error = function(e) NULL)

      like_slot <- tryCatch(as.numeric(slot(modules_result, "likelihood")), error = function(e) NA_real_)

      if (is.na(modularity_val)) modularity_val <- like_slot



      if (is.list(mods_slot)) {
        # Flatten potential named or nested structures

        candidates <- unlist(mods_slot, recursive = FALSE)

        vecs <- Filter(is.numeric, candidates)


        # Prefer exact matches by length

        if (length(vecs) >= 2) {
          match_fg <- which(vapply(vecs, length, 1L) == length(fg_names))

          match_py <- which(vapply(vecs, length, 1L) == length(pyov_names))



          if (length(match_fg) >= 1 && length(match_py) >= 1) {
            fg_modules <- as.integer(vecs[[match_fg[1]]])

            pyov_modules <- as.integer(vecs[[match_py[1]]])
          } else {
            # As a last resort, assume first two numeric vectors correspond to row/col memberships
            fg_modules <- as.integer(vecs[[1]])

            pyov_modules <- as.integer(vecs[[2]])
          }
        }
      }
    }

    # Tertiary path: weak fallback hint if moduleWeb present but no vectors could be extracted
    if (is.null(fg_modules) || is.null(pyov_modules)) {
      mw <- tryCatch(slot(modules_result, "moduleWeb"), error = function(e) NULL)

      if (!is.null(mw) && (is.matrix(mw) || is.data.frame(mw))) {
        used_fallback <- TRUE
      }
    }
  }


  # Case B: List-like object with "modules" and possibly "likelihood"
  if (is.null(fg_modules) && is.list(modules_result)) {
    if ("modules" %in% names(modules_result)) {
      mods <- modules_result$modules
      if (is.list(mods)) {
        # Prefer vectors that match lengths
        vecs <- Filter(is.numeric, mods)
        if (length(vecs) >= 2) {
          match_fg <- which(vapply(vecs, length, 1L) == length(fg_names))
          match_py <- which(vapply(vecs, length, 1L) == length(pyov_names))
          if (length(match_fg) >= 1 && length(match_py) >= 1) {
            fg_modules <- as.integer(vecs[[match_fg[1]]])
            pyov_modules <- as.integer(vecs[[match_py[1]]])
          } else {
            fg_modules <- as.integer(vecs[[1]])
            pyov_modules <- as.integer(vecs[[2]])
          }
        }
      }
    }
    if ("likelihood" %in% names(modules_result) && is.numeric(modules_result$likelihood)) {
      modularity_val <- as.numeric(modules_result$likelihood)
    } else if ("Q" %in% names(modules_result) && is.numeric(modules_result$Q)) {
      modularity_val <- as.numeric(modules_result$Q)
    } else if ("modularity" %in% names(modules_result) && is.numeric(modules_result$modularity)) {
      modularity_val <- as.numeric(modules_result$modularity)
    }
  }

  # Case C: Atomic vector representing combined membership
  if (is.null(fg_modules) && is.numeric(modules_result)) {
    total_expected <- length(fg_names) + length(pyov_names)
    if (length(modules_result) == total_expected) {
      fg_modules <- as.integer(modules_result[seq_len(length(fg_names))])
      pyov_modules <- as.integer(modules_result[(length(fg_names) + 1):total_expected])
    }
  }


  # Reorder memberships by provided names if present
  if (!is.null(names(fg_modules))) {
    fg_modules <- fg_modules[fg_names]
  }
  if (!is.null(names(pyov_modules))) {
    pyov_modules <- pyov_modules[pyov_names]
  }

  # Coerce to compact positive integer labels (1..K)
  fg_modules <- as.integer(factor(fg_modules, levels = unique(fg_modules)))
  pyov_modules <- as.integer(factor(pyov_modules, levels = unique(pyov_modules)))

  # Validate and fallback if needed
  if (length(fg_modules) != length(fg_names) || any(is.na(fg_modules)) ||

    length(pyov_modules) != length(pyov_names) || any(is.na(pyov_modules))) {
    fg_modules <- rep(1L, length(fg_names))

    pyov_modules <- rep(1L, length(pyov_names))

    modularity_val <- ifelse(is.na(modularity_val), 0, modularity_val)

    used_fallback <- TRUE
  }



  # Finalize
  fg_modules <- as.integer(pmax(1L, fg_modules))

  pyov_modules <- as.integer(pmax(1L, pyov_modules))

  n_modules <- max(c(fg_modules, pyov_modules), na.rm = TRUE)


  list(
    fg_modules = fg_modules,
    pyov_modules = pyov_modules,
    modularity = modularity_val,
    n_modules = n_modules,
    success = !used_fallback
  )
}

# -----------------------------
# Directory setup
# -----------------------------
safe_dir_create("results/phase_04/modularity")
safe_dir_create("figures/network_topology")
safe_dir_create("docs/phase_04/logs")

cat("=== Phase 04 - Step 03: Bipartite Modularity Analysis ===\n")
cat(paste("Timestamp:", timestamp(), "\n\n"))

# -----------------------------
# Load parameters
# -----------------------------
manifest_json <- "docs/phase_04/parameters_manifest.json"
manifest <- list()
if (file.exists(manifest_json)) {
  manifest <- tryCatch(fromJSON(manifest_json), error = function(e) {
    message("Could not parse manifest JSON; using defaults")
    list()
  })
}

# Parameters with manifest overrides
master_seed <- if (!is.null(manifest$master_seed)) manifest$master_seed else 2025
mod_reps_pilot <- if (!is.null(manifest$modularity_replicates$pilot)) manifest$modularity_replicates$pilot else 20
mod_reps_final <- if (!is.null(manifest$modularity_replicates$final)) manifest$modularity_replicates$final else 100

# Use final replicates for production run
n_replicates <- mod_reps_final

cat(sprintf("Parameters: master_seed=%d, n_replicates=%d\n", master_seed, n_replicates))

# -----------------------------
# Load inputs and build incidence matrix
# -----------------------------
cat("Loading adjacency matrices and building bipartite incidence matrix...\n")

# Load adjacency matrices
adj_prod_fg <- readRDS("data/interim/adj_production_FG_agentsxpyov_conservative.rds") # FG x pyov
adj_util_fg <- readRDS("data/interim/adj_utilization_FG_pyovxagents_conservative.rds") # pyov x FG

# Load node tables
fg_nodes <- readRDS("data/interim/nodes_functional_groups_conservative.rds")
pyov_nodes <- readRDS("data/interim/nodes_pyoverdines_conservative.rds")

# Build bipartite incidence matrix (FG x pyov)
# An FG is connected to a pyov if it produces OR utilizes it
adj_util_reoriented <- t(adj_util_fg) # Convert pyov x FG to FG x pyov
incidence_matrix <- (adj_prod_fg > 0) | (adj_util_reoriented > 0)
incidence_matrix <- as.matrix(incidence_matrix) * 1 # Convert to numeric 0/1

# Get node names
fg_names <- rownames(incidence_matrix)
pyov_names <- colnames(incidence_matrix)

# Basic network statistics
n_fg <- nrow(incidence_matrix)
n_pyov <- ncol(incidence_matrix)
n_interactions <- sum(incidence_matrix)
network_density <- n_interactions / (n_fg * n_pyov)

cat(sprintf(
  "Incidence matrix: %d FGs x %d pyovs, %d interactions (%.1f%% density)\n",
  n_fg, n_pyov, n_interactions, 100 * network_density
))

# Validate matrix has sufficient structure for modularity
if (n_interactions < 2 || n_fg < 2 || n_pyov < 2) {
  stop("Insufficient network structure for modularity analysis")
}

# -----------------------------
# Replicate bipartite modularity analysis
# -----------------------------
cat("Running bipartite modularity analysis with replicates...\n")

# Generate deterministic seeds
set.seed(master_seed)
modularity_task_seed <- master_seed + 2 # From manifest task_index_mapping
set.seed(modularity_task_seed)
replicate_seeds <- sample.int(1e6, n_replicates)

# Storage for results
fg_partitions <- matrix(NA_integer_,
  nrow = n_fg, ncol = n_replicates,
  dimnames = list(fg_names, paste0("rep_", seq_len(n_replicates)))
)
pyov_partitions <- matrix(NA_integer_,
  nrow = n_pyov, ncol = n_replicates,
  dimnames = list(pyov_names, paste0("rep_", seq_len(n_replicates)))
)
modularity_scores <- numeric(n_replicates)
n_modules_vec <- integer(n_replicates)
success_flags <- logical(n_replicates)

# Run replicates
for (r in seq_len(n_replicates)) {
  seed_r <- replicate_seeds[r]
  set.seed(seed_r)

  cat(sprintf("Replicate %d/%d (seed=%d)... ", r, n_replicates, seed_r))

  # Add small random noise (some versions treat web as binary; noise may be ignored)
  incidence_noisy <- (incidence_matrix > 0) * 1L

  # Introduce replicate-specific row/column permutations as stochasticity
  row_perm <- sample.int(n_fg)
  col_perm <- sample.int(n_pyov)
  inv_row <- integer(n_fg)
  inv_row[row_perm] <- seq_len(n_fg)
  inv_col <- integer(n_pyov)
  inv_col[col_perm] <- seq_len(n_pyov)

  incidence_perm <- incidence_noisy[row_perm, col_perm, drop = FALSE]


  modules_result <- tryCatch(
    {
      bipartite::computeModules(as.matrix(incidence_perm * 1), method = "Beckett")
    },
    error = function(e) {
      warning(sprintf("computeModules failed on replicate %d: %s", r, e$message))

      NULL
    }
  )


  # Extract module assignments (these are in permuted order)
  extracted <- extract_bipartite_modules(modules_result, fg_names[row_perm], pyov_names[col_perm])

  # Map module memberships back to original row/col order
  fg_modules_orig <- integer(n_fg)
  fg_modules_orig[row_perm] <- extracted$fg_modules
  pyov_modules_orig <- integer(n_pyov)
  pyov_modules_orig[col_perm] <- extracted$pyov_modules

  # Store results
  fg_partitions[, r] <- fg_modules_orig
  pyov_partitions[, r] <- pyov_modules_orig
  modularity_scores[r] <- ifelse(is.na(extracted$modularity), 0, extracted$modularity)
  n_modules_vec[r] <- max(c(fg_modules_orig, pyov_modules_orig), na.rm = TRUE)
  success_flags[r] <- extracted$success

  if (r %% 10 == 0 || r <= 5) {
    cat(sprintf(
      "modules=%d, Q=%.3f, success=%s\n",
      n_modules_vec[r], modularity_scores[r], success_flags[r]
    ))
  } else {
    cat("✓\n")
  }
}

# Filter to successful replicates
valid_reps <- which(success_flags)
n_valid <- length(valid_reps)

if (n_valid == 0) {
  stop("No successful modularity replicates. Check bipartite package installation and data.")
}

if (n_valid < n_replicates) {
  cat(sprintf("Warning: Only %d/%d replicates succeeded\n", n_valid, n_replicates))
  fg_partitions <- fg_partitions[, valid_reps, drop = FALSE]
  pyov_partitions <- pyov_partitions[, valid_reps, drop = FALSE]
  modularity_scores <- modularity_scores[valid_reps]
  n_modules_vec <- n_modules_vec[valid_reps]
}

# -----------------------------
# Compute partition stability (ARI analysis)
# -----------------------------
cat("Computing partition stability across replicates...\n")

# Combined partitions for ARI calculation
combined_partitions <- rbind(fg_partitions, pyov_partitions)
all_node_names <- c(fg_names, pyov_names)

# Compute pairwise ARI matrix
ari_matrix <- matrix(NA_real_, nrow = n_valid, ncol = n_valid)
for (i in seq_len(n_valid)) {
  for (j in i:n_valid) {
    ari_val <- compute_ari(combined_partitions[, i], combined_partitions[, j])
    ari_matrix[i, j] <- ari_val
    ari_matrix[j, i] <- ari_val
  }
}

# Compute ARI summary statistics
ari_values <- ari_matrix[upper.tri(ari_matrix)]
mean_ari <- mean(ari_values, na.rm = TRUE)
median_ari <- median(ari_values, na.rm = TRUE)
sd_ari <- sd(ari_values, na.rm = TRUE)

cat(sprintf(
  "Partition stability: mean ARI=%.3f, median=%.3f, SD=%.3f\n",
  mean_ari, median_ari, sd_ari
))

# -----------------------------
# Consensus partition using medoid approach
# -----------------------------
cat("Computing consensus partition using medoid approach...\n")

medoid_result <- find_medoid_partition(combined_partitions)
medoid_idx <- medoid_result$medoid_idx
avg_ari_to_medoid <- medoid_result$avg_ari_to_medoid

# Extract consensus partition
consensus_fg_modules <- fg_partitions[, medoid_idx]
consensus_pyov_modules <- pyov_partitions[, medoid_idx]
consensus_modularity <- modularity_scores[medoid_idx]
consensus_n_modules <- n_modules_vec[medoid_idx]

names(consensus_fg_modules) <- fg_names
names(consensus_pyov_modules) <- pyov_names

cat(sprintf(
  "Consensus partition: medoid replicate %d, %d modules, Q=%.3f, avg_ARI=%.3f\n",
  medoid_idx, consensus_n_modules, consensus_modularity, avg_ari_to_medoid
))

# -----------------------------
# Module characterization
# -----------------------------
cat("Characterizing module composition...\n")

# Create module summary table
module_summary <- data.frame(
  module = seq_len(consensus_n_modules),
  n_fg = integer(consensus_n_modules),
  n_pyov = integer(consensus_n_modules),
  n_total = integer(consensus_n_modules),
  density = numeric(consensus_n_modules),
  stringsAsFactors = FALSE
)

for (m in seq_len(consensus_n_modules)) {
  fg_in_module <- which(consensus_fg_modules == m)
  pyov_in_module <- which(consensus_pyov_modules == m)

  module_summary$n_fg[m] <- length(fg_in_module)
  module_summary$n_pyov[m] <- length(pyov_in_module)
  module_summary$n_total[m] <- module_summary$n_fg[m] + module_summary$n_pyov[m]

  # Calculate module density (interactions within module / possible interactions)
  if (length(fg_in_module) > 0 && length(pyov_in_module) > 0) {
    module_submatrix <- incidence_matrix[fg_in_module, pyov_in_module, drop = FALSE]
    module_summary$density[m] <- sum(module_submatrix) / (length(fg_in_module) * length(pyov_in_module))
  } else {
    module_summary$density[m] <- 0
  }
}

module_summary <- module_summary[order(module_summary$n_total, decreasing = TRUE), ]

# -----------------------------
# Save results
# -----------------------------
cat("Saving modularity results...\n")

# Replicate results
replicates_data <- list(
  fg_partitions = fg_partitions,
  pyov_partitions = pyov_partitions,
  modularity_scores = modularity_scores,
  n_modules_vec = n_modules_vec,
  success_flags = success_flags,
  replicate_seeds = replicate_seeds[valid_reps],
  n_replicates = n_valid,
  metadata = list(
    master_seed = master_seed,
    modularity_task_seed = modularity_task_seed,
    timestamp = timestamp(),
    incidence_dimensions = c(n_fg, n_pyov),
    network_density = network_density
  )
)
saveRDS(replicates_data, "results/phase_04/modularity/modules_fg_replicates.rds")

# Consensus results
consensus_data <- list(
  fg_modules = consensus_fg_modules,
  pyov_modules = consensus_pyov_modules,
  modularity = consensus_modularity,
  n_modules = consensus_n_modules,
  medoid_replicate = medoid_idx,
  avg_ari_to_medoid = avg_ari_to_medoid,
  module_summary = module_summary,
  metadata = list(
    consensus_method = "medoid_partition",
    timestamp = timestamp()
  )
)
saveRDS(consensus_data, "results/phase_04/modularity/modules_fg_consensus.rds")

# Stability metrics
stability_data <- list(
  ari_matrix = ari_matrix,
  ari_summary = list(
    mean = mean_ari,
    median = median_ari,
    sd = sd_ari,
    min = min(ari_values, na.rm = TRUE),
    max = max(ari_values, na.rm = TRUE)
  ),
  modularity_summary = list(
    mean = mean(modularity_scores),
    median = median(modularity_scores),
    sd = sd(modularity_scores),
    min = min(modularity_scores),
    max = max(modularity_scores)
  ),
  n_modules_summary = list(
    mean = mean(n_modules_vec),
    median = median(n_modules_vec),
    sd = sd(n_modules_vec),
    min = min(n_modules_vec),
    max = max(n_modules_vec)
  ),
  metadata = list(
    n_valid_replicates = n_valid,
    n_total_replicates = n_replicates,
    timestamp = timestamp()
  )
)
saveRDS(stability_data, "results/phase_04/modularity/modularity_stability.rds")

# -----------------------------
# Generate visualizations
# -----------------------------
cat("Generating modularity visualizations...\n")

# Modularity and module count distributions
stability_df <- data.frame(
  replicate = seq_len(n_valid),
  modularity = modularity_scores,
  n_modules = n_modules_vec,
  ari_to_consensus = sapply(seq_len(n_valid), function(i) {
    if (i == medoid_idx) {
      return(1.0)
    }
    compute_ari(combined_partitions[, i], combined_partitions[, medoid_idx])
  })
)

p_stability <- stability_df %>%
  pivot_longer(cols = c(modularity, n_modules), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 20, fill = "steelblue", alpha = 0.7) +
  facet_wrap(~metric, scales = "free", labeller = labeller(metric = c(
    modularity = "Modularity (Q)",
    n_modules = "Number of Modules"
  ))) +
  geom_vline(data = data.frame(
    metric = c("modularity", "n_modules"),
    consensus_val = c(consensus_modularity, consensus_n_modules)
  ), aes(xintercept = consensus_val), color = "red", linetype = "dashed") +
  labs(
    title = "Bipartite Modularity: Replicate Distributions",
    subtitle = sprintf(
      "Consensus (red line): Q=%.3f, modules=%d, mean ARI=%.3f",
      consensus_modularity, consensus_n_modules, avg_ari_to_medoid
    ),
    x = "Value", y = "Frequency"
  ) +
  theme_minimal()

ggsave("figures/network_topology/modularity_stability_metrics.pdf", p_stability, width = 10, height = 6)

# Module composition visualization
p_modules <- module_summary %>%
  mutate(module = factor(module, levels = module)) %>%
  pivot_longer(cols = c(n_fg, n_pyov), names_to = "node_type", values_to = "count") %>%
  ggplot(aes(x = module, y = count, fill = node_type)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(
    values = c(n_fg = "lightblue", n_pyov = "orange"),
    labels = c(n_fg = "Functional Groups", n_pyov = "Pyoverdines")
  ) +
  labs(
    title = "Bipartite Module Composition (Consensus)",
    subtitle = sprintf("%d modules, Q=%.3f", consensus_n_modules, consensus_modularity),
    x = "Module", y = "Node Count", fill = "Node Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggsave("figures/network_topology/modularity_bipartite_consensus.pdf", p_modules, width = 8, height = 6)

# -----------------------------
# Generate summary report
# -----------------------------
cat("Generating summary report...\n")

summary_lines <- c(
  "Phase 04 - Step 03: Bipartite Modularity Analysis Summary",
  paste("Timestamp:", timestamp()),
  "",
  "Method: bipartite::computeModules() with medoid consensus",
  sprintf("Replicates: %d successful / %d total", n_valid, n_replicates),
  sprintf("Master seed: %d (modularity task seed: %d)", master_seed, modularity_task_seed),
  "",
  "Network Structure:",
  sprintf("  Incidence matrix: %d FGs × %d pyoverdines", n_fg, n_pyov),
  sprintf("  Total interactions: %d (%.1f%% density)", n_interactions, 100 * network_density),
  "",
  "Modularity Results:",
  sprintf("  Consensus modularity (Q): %.4f", consensus_modularity),
  sprintf("  Consensus modules: %d", consensus_n_modules),
  sprintf("  Medoid replicate: %d (avg ARI to consensus: %.3f)", medoid_idx, avg_ari_to_medoid),
  "",
  "Replicate Statistics:",
  sprintf(
    "  Modularity: mean=%.3f ± %.3f, range=[%.3f, %.3f]",
    mean(modularity_scores), sd(modularity_scores),
    min(modularity_scores), max(modularity_scores)
  ),
  sprintf(
    "  Module count: mean=%.1f ± %.1f, range=[%d, %d]",
    mean(n_modules_vec), sd(n_modules_vec),
    min(n_modules_vec), max(n_modules_vec)
  ),
  "",
  "Partition Stability:",
  sprintf("  Mean pairwise ARI: %.3f ± %.3f", mean_ari, sd_ari),
  sprintf("  ARI range: [%.3f, %.3f]", min(ari_values, na.rm = TRUE), max(ari_values, na.rm = TRUE)),
  if (mean_ari > 0.7) "  → High stability (ARI > 0.7)" else if (mean_ari > 0.3) "  → Moderate stability (0.3 < ARI < 0.7)" else "  → Low stability (ARI < 0.3)",
  "",
  "Module Composition (Consensus):",
  paste(sprintf(
    "  Module %d: %d FGs, %d pyovs, density=%.2f",
    module_summary$module, module_summary$n_fg,
    module_summary$n_pyov, module_summary$density
  ), collapse = "\n"),
  "",
  "Interpretation:",
  if (consensus_modularity > 0.3) "  • Strong modular structure detected" else if (consensus_modularity > 0.1) "  • Moderate modular structure" else "  • Weak or no modular structure",
  if (mean_ari > 0.5) "  • Stable partition across replicates" else "  • Unstable partitioning - interpret with caution",
  "",
  "Outputs:",
  "  • results/phase_04/modularity/modules_fg_replicates.rds",
  "  • results/phase_04/modularity/modules_fg_consensus.rds",
  "  • results/phase_04/modularity/modularity_stability.rds",
  "  • figures/network_topology/modularity_bipartite_consensus.pdf",
  "  • figures/network_topology/modularity_stability_metrics.pdf",
  "",
  "Next Steps:",
  "  • Run null model testing if modularity appears significant",
  "  • Proceed to nestedness analysis (Step 04)",
  "  • Consider biological interpretation of modules"
)

writeLines(summary_lines, "docs/phase_04/logs/step03_modularity_bipartite_summary.txt")

# Save session info
writeLines(capture.output(sessionInfo()), "docs/phase_04/logs/step03_session_info.txt")

# Display summary
cat(paste(summary_lines, collapse = "\n"), "\n")

cat("\nStep 03 complete. Bipartite modularity analysis finished.\n")
cat("Consensus modularity Q =", round(consensus_modularity, 3), "\n")
cat("Partition stability (mean ARI) =", round(mean_ari, 3), "\n")
